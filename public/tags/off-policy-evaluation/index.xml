<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Off-Policy Evaluation on 运筹OR帷幄｜讲座汇总</title><link>http://localhost:1313/tags/off-policy-evaluation/</link><description>Recent content in Off-Policy Evaluation on 运筹OR帷幄｜讲座汇总</description><generator>Hugo 0.125.0</generator><language>zh-cn</language><copyright>运筹OR帷幄</copyright><lastBuildDate>Wed, 17 Apr 2024 21:01:06 -0400</lastBuildDate><atom:link href="http://localhost:1313/tags/off-policy-evaluation/index.xml" rel="self" type="application/rss+xml"/><item><title>Tech Talk 41: 当A/B实验平台遇上强化学习</title><link>http://localhost:1313/techtalks/techtalk_41.zh/</link><pubDate>Wed, 17 Apr 2024 21:01:06 -0400</pubDate><guid>http://localhost:1313/techtalks/techtalk_41.zh/</guid><description>题目: 直播回顾｜哥大 彭天翼：当A/B实验平台遇上强化学习 「TechTalk 41」 封面：
摘要： 关键词：A/B测试；实验干扰(interference)；强化学习
嘉宾介绍： 彭天翼 哥伦比亚大学商学院决策风险与运筹系助教教授（拟入职）。他于2023年取得麻省理工学院博士学位，2017年毕业于清华大学姚班。现于Cimulate.AI担任首席人工智能研究员。他的研究兴趣聚焦于生成式人工智能，强化学习，和因果推断。他喜欢关注前沿理论问题在实际问题中的应用，并曾与字节跳动，百威啤酒，the Broad Institute等公司展开合作。他曾获得INFORMS Daniel H. Wagner实践卓越奖，Applied Probability Society Best Student Prize，RMP Jeff McGill Student Paper Award，和MSOM Best Student Prize Finalist。
分享内容： 以上是我们对本次直播展示 PPT 的部分截取。 关注下方公众号，后台回复关键词 TechTalk，可获取本期录播和 PPT 的完整内容。
观众提问： 一 Q: A/B测试是否可以扩展到A/B/C/D等多组场景？ A: 可以。方法论上具有很强的可扩展性，但是需要修改相应的数学描述。
二 Q: A/A测试中的两个A是指不同版本间的比较吗？ A: 不是，A/A测试是对于同一个算法的分流测试。在这种情况下，真实的treatment effect是0。它一般用于检查estimator的鲁棒性或者估计estimator的方差。A/A测试可以对用户分成两组测试，有时也会分成两个时间段测试。
三 Q: 关于DQ方法的灵感/直觉来源什么？ A: 从RL的角度来看，interference的产生主要是由于当下的action会影响到未来的reward。所以我们消除interference的方法就采用了未来的reward总和（也就是Q-value）来代替当下的reward去计算两组实验的差值。我们在抖音场景下举了两个例子来进一步阐释，具体内容欢迎观看video或者阅读论文。 &amp;hellip; 详细解答以及大量关于 A/B测试/强化学习/Interference 的应用讨论（“坑”）请移步B站录播视频，自行食用。 相关资料：
Farias, Vivek, Andrew Li, Tianyi Peng, and Andrew Zheng. &amp;ldquo;Markovian interference in experiments.</description></item></channel></rss>